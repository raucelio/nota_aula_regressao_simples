[
["index.html", "Regressão Linear Objetivo", " Regressão Linear Objetivo Este material é uma pequena introdução à Regressão Linear Simples e como executar essa análise no R. Será introduzido de maneira breve a teoria de estimação dos parâmetro e os testes de hipóteses que são realizados para atestar se realmente são significantes, isto é, diferentes de zero. Um estudo na função lm(), usada para ajustar um modelos lineares, e como obter outros resultados importantes do do modelo ajustdado. Finalmente, serão apresentados exercícios e exemplos de regressões lineares simple, os quais têm niveis de dificuldade crescente. Boa Sorte. "],
["intro.html", "Capítulo 1 Introdução 1.1 Ajuste do modelo 1.2 Inferência em regressão linear", " Capítulo 1 Introdução Francis Galton no artigo Regression towards Mediocrity in Hereditary Stature publicado no Journal of the Anthropological Institute em 1886 cunhou o termo regressão (Galton 1886). Este artigo evidência que a altura média dos filhos de pais de uma dada altura tendia a se deslocar ou “regredir” até a altura média da população. Em outras palavras, a altura dos filhos de pais extraordinariamente altos ou baixos tende a se mover para a altura média da população. A regressão procura estudar a relação de uma variável, chamada dependente, em relação a uma ou mais variáveis, as variáveis independentes. O seu principal objetivo de estimar a média da população em termos dos valores conhecidos ou fixos (em amostragem repetida) das variáveis independentes. Este texto apresenta como fazer uma análise de regressão linear simples no R e para isso é apresentado de forma breve os conceitos de Ajuste pelo método do mínimos quadrados; Decomposição da soma dos quadrados total; Coeficiente de determinação e coeficiente de determinação ajustad; e ANOVA para modelos de regressão simples. 1.1 Ajuste do modelo Em (Nelson 2004) o termo regressão é definido como um modelo estatístico para previsão do valor médio de uma variável aleatória (v.a.) quando outras variáveis, uma ou mais, assumem um valor fixo. Pode-se citar as seguintes situações como exemplo: a equação que retorna as distâncias médias entre as paradas dos carros em uma viagem para uma deteminada velocidade, ou a equação que relaciona o peso médio de uma criança de uma certa idade. Mais formalmente, seja o valor médio da v.a. Y para um dado valor de X e denotado por \\(E(Y|X)\\). Logo, a regressão de Y em X é uma equação expressa \\(E(Y|X)\\) em termos de X. Essa equação é chamada de equação de regressão e prediz o valor médio de Y para um dado valor X. Nesse contexto: A v.a. Y é a variavel dependente (ou resposta). A variável X é a variável independente (ou explanatória). \\(E(Y|X)\\) é poder ser abreviada por \\(y_x\\) ou Y. Uma regressão linear simples ocorre quando o valor médio de \\(Y\\) é função linear de apenas uma varíavel \\(X\\), assim \\[ E(Y|X) = \\beta_o + \\beta_1 x, \\] onde \\(\\beta_0\\) e \\(\\beta_1\\) são parâmetros e designaodos por coeficientes de regressão, e a principal tarefa da análise de regressão é estimar valores para estes parâmetros a partir de dados amostrais. Se Y apresentar uma distribuição normal com variância constante \\(\\sigma^2\\) para cada valor de X, para uma amostra de valores de n pares \\((x_i, y_i)\\) independentes, o método de Mínimos Quadrados Ordinários dão as melhores estimativas de \\(\\beta_0\\) e \\(\\beta_1\\). As estimativas \\(\\hat{\\beta_0}\\) e \\(\\hat{\\beta_1}\\) dos parâmetros \\(\\beta_0\\) e \\(\\beta_1\\) fornecidas pelo método dos mínimos quadrados ordinários podem ser obtidas na maioria das calculadoras e dos pacotes estatísticos. Eles, também, podem ser calculados diretamente pelas fórmulas: \\[ \\hat{\\beta_1}=\\dfrac{n\\sum_i x_iy_i - \\sum_i x_i \\sum_i y_i}{n\\sum_i x_i^2 - \\left(\\sum_i x_i\\right)^2} \\] e \\[ \\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}, \\] onde \\[ \\bar{x}=\\dfrac{\\sum_i x_i}{n} \\] e \\[ \\bar{y}=\\dfrac{\\sum_i y_i}{n} \\] A reta com a equação \\(Y= \\beta_0+\\beta_1X\\) é chamada regressão linear de Y em X. 1.1.1 Peso e Altura de Mulheres O data frame women tráz o peso médio e a altura de 15 (quinze) mulheres americanas com idades entre 30 e 39 anos. A variável height (altura) está em polegadas e variável weight (peso) está em libras. O gráfico de dispersão indica a existência de uma relação linear entre a varíavel weigth e heigth. library(ggplot2) ggplot(women, aes(x=height, y=weight)) + geom_point()+ labs(x=&quot;Altura em polegadas.&quot;, y=&quot;Peso em libras&quot;)+ theme_classic() O código a seguir utiliza as fórmulas para os estimadores \\(\\hat{\\beta}_0\\) e \\(\\hat{\\beta}_1\\) para encontrar as respectivas estimatias. n &lt;- nrow(women) X &lt;- women$height Y &lt;- women$weight Sx &lt;- sum(X) Sx_quadrado &lt;-sum (X^2) Sy &lt;- sum(Y) Sxy &lt;- sum(X*Y) x_barra &lt;- mean(X) y_barra &lt;- mean(Y) beta1 &lt;- (n* Sxy - Sx*Sy)/ (n*Sx_quadrado - (Sx)^2) beta0 &lt;- y_barra - beta1*x_barra Assim, com \\(\\hat{\\beta}_0 = -87.52\\) e \\(\\hat{\\beta}_0 = 3.45\\), o modelo proposto é: \\[ weigth = -87.52 + 3.45heigth \\] 1.2 Inferência em regressão linear Uma alternativa, equivalente formulação da média da regressão da equação é \\[ Y = \\beta_0 + \\beta_1 x + \\epsilon, \\] onde \\(\\epsilon\\) é um componente aleatório, independente de X, tal que \\(\\epsilon \\sim N(0, \\sigma^2 )\\), isto é, tem distribuição normal com média zero e variância constante \\(\\sigma^2\\). Para ser realizada inferência sobre os parâmetros e os valores preditos é necessário que sejam satisfeitos os seguintes pressupostos: A relação entre X e Y é linear. Os valores de X são fixos, isto é, X não é uma variável aleatória. Os erros tém média igual a 0 (zero), \\(E(e_i)=0\\). Para um dado valor de X, a variância dos erros é sempre \\(\\sigma^2\\). O erro de uma observação é idependente do erro de outra observação, \\(E(e_ie_j)=0\\) para \\(\\forall i \\neq j\\). Os erros tem distrbuição normal. Combinando os pressupostos 3, 4, 5 e 6 tem-se que os erros não Normalmente e Independentemente Distribuidos com média 0 (zero) e variância \\(\\sigma^2\\). As pressuposições 1, 2 e 3 permitem escrever \\(E(Y) = \\beta_0 + \\beta_1X\\), isto é, a s média das distribuições de \\(Y|X\\) estão sobre a reta \\(\\beta_0 + \\beta_1X\\). O pressuposto 4 faz com que \\(Y|X\\) tenham a mesma variância \\(\\sigma^2\\), isto é, os valores de Y relacionados a diferentes valores de X apresentam a mesma variância. A pressuposição 6 é necessária para que se posssa utilizar as distribuições t e F, a fim de testar hipóteses ou construir intervalos de confiança. No próximo tópico, será apresentada a decomposição da soma dos quadrado total para para justificar a construção de uma tabela ANOVA, na qual há um teste F, que procura verificar se o parãmetro \\(\\beta_1\\) é significativo. 1.2.1 Decomposião da Soma dos Quadrados Total A Soma dos Quatrados Total (SQT) é definida para uma amostra de n pares ordenados \\((x_i,y_i)\\), com \\(i = 1,2 , \\ldots, n\\), pela fórmula: \\[ SQT = \\sum_{i=1}^y \\left( y_i - \\bar{y}\\right)^2, \\] onde \\(\\bar{y}\\) é a média dos valores observados em \\(y\\). Sendo \\(\\hat{y_i}\\) o valor predito pelo modelo de regressão linear simples e partindo da identidade \\(y_i - \\bar{y}_i = \\hat{y} - \\bar{y} + y_i - \\bar{y}_i\\), é possivel dividir a SQT em duas parcelas: A Somas dos Quadrados da Regressão (SQReg) dada por: \\[ SQT = \\sum_{i=1}^y \\left( \\hat{y}_i - \\bar{y}\\right)^2 \\] + A Soma dos Quadrados dos Resíduos dado por : \\[ SQT = \\sum_{i=1}^y \\left( y_i - \\hat{y}\\right)^2, \\] Assim, tem-se a igualdade \\(SQT = SQReg + SQRes\\) ou, de outra forma, \\[ \\sum_{i=1}^y \\left( y_i - \\bar{y}\\right)^2 = \\sum_{i=1}^y \\left( \\hat{y}_i - \\bar{y}\\right)+ \\sum_{i=1}^y \\left( y_i - \\hat{y}_i\\right)^2 \\] ### Exemplo Seja uma amostra de seis pares de valores. X Y 1 3,0 3 8,0 4 7,0 5 11,0 6 10,0 8 12,0 X &lt;- c(1,3,4,5,6,8) Y &lt;- c(3.0, 8.0, 7.0, 11.0, 10.0, 12.0) Admitindo que X e Y se realcionam pelo modelo \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\), onde \\(e_i\\) são independentes com distribuição normal com média zero e variãncia $^2. Determine as estimativas do modelo. Utilizando o método dos mínimos quadrados ordinários tem-se que \\[ \\hat{\\beta_1}=\\dfrac{n\\sum_i x_iy_i - \\sum_i x_i \\sum_i y_i}{n\\sum_i x_i^2 - \\left(\\sum_i x_i\\right)^2} \\] e \\[ \\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}, \\] Assim, n &lt;- length(X) Sx &lt;- sum(X) Sx_quadrado &lt;-sum (X^2) Sy &lt;- sum(Y) Sxy &lt;- sum(X*Y) x_barra &lt;- mean(X) y_barra &lt;- mean(Y) beta1 &lt;- (n* Sxy - Sx*Sy)/ (n*Sx_quadrado - (Sx)^2) beta0 &lt;- y_barra - beta1*x_barra Logo, \\(\\hat{\\beta_0} = 2.93\\) e \\(\\hat{\\beta_1} = 1.24\\). Obtenha a soma dos quadrados Y_chapeu &lt;- beta0 + beta1*X SQT &lt;- sum((Y - y_barra)^2) SQReg &lt;- sum((Y_chapeu - y_barra)^2) SQRes &lt;- sum((Y - Y_chapeu)^2) Logo, SQT = 53.5 SQReg = 45.16 SQRes = 8.34 Com estimativas SQT e SQReg pode-se obter uma medida de qualidade do modelo. Esta médida é o \\(R^2\\), conhecido por coeficiente de determinação, que é definido por \\[ R^2 = \\dfrac{SQReg}{SQT}. \\] O \\(R^2\\) é a proporção da variãncia explicada pela regressão. O seu valor varia de 0 (zero) a 1(um) e quanto maior o seu valor melhor. Com os dados utilizados tem-se que \\(R^2 = 0.84\\). No caso de uma regressão linear simples, o \\(R^2\\) pode diminuir com o aumento do número de observações. Para contornar esse incoveniente, foi definido coeficeinte de determinação corrigido \\[ \\bar{R}^2 = R^2 - \\dfrac{1}{n-2}(1- R^2), \\] onde n é quantidade de observações na amostra. Para os dados do exemplo, tem-se \\(\\bar{R}^2 = 0.81\\). É importante notar: \\(\\bar{R}^2 &lt; R^2\\) e \\(\\bar{R}^2\\) pode assumir um valor negativo. Valor Esperado para as Somas dos Quadrados As somas dos quadrados sendo estimativas e com valores dependentes da amostra analisada, tém distribuição de probabilidade, portanto, é possível se obter para cada soma um seu valor esperado. Os graus de liberdade e valores esperados dos estimadores dessas somas de quadrados que estão na tabela abaixo: SQ Valor Esperado - E(SQ) Graus de Liberdade - gl : SQReg \\(\\sigma^2 + \\beta_1^2 + \\sum x_i^2\\) 1 SQres \\((n-2)\\sigma^2\\) n-2 SQT \\((n-1)\\sigma^2 + \\beta_1^2 + \\sum x_i^2\\) n-1 Quadrados médios A fim de construir uma ANOVA são necessários os Quadrados Médios (QM) da SQReg e dos SQRes, que são SReg e SQRes divididos pelos respectivos graus de liberdade. SQ gl QM Valor Esperado - E(QM) SQReg 1 \\(\\dfrac{SQReg}{1}\\) \\(\\sigma^2 + \\beta_1^2 + \\sum x_i^2\\) SQres n-2 \\(\\dfrac{SQRes}{n-2}\\) \\(\\dfrac{(n-2)\\sigma^2}{(n-2)}= \\sigma^2\\) ANOVA Os resultados acima podem ser organizados de forma a gerar uma análise de variância para o modelo de regressão linear: Fonte de variação FV gl SQ QM F Regressão 1 \\(\\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2\\) \\(\\dfrac{\\sum_{i=1}^n (\\hat{Y}_i - \\bar{y})^2}{1}\\) \\(\\dfrac{QMReg}{QMRes}\\) Resíduo n-2 \\(\\sum_{i=1}^n (y_i - \\hat{y}_i )^2\\) \\(\\dfrac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n-2}\\) Tital n-1 1.2.2 Teste t para os parâmetros É possível realizar inferência sobre os parãometros estimados e sobre os valores preditos. Para isso é necessário obter a variâncias dessas estimativas. Se os pressupostos dos modelo forem atendidos, pode-se demostrar essas variancias estimadas são: Variância de \\(\\hat(\\beta_0)\\) \\[ Var(\\hat\\beta_0)= \\left( \\dfrac{1}{n}+ \\dfrac{\\bar{X}}{\\sum x_i ^2} \\right) \\] + Variância de \\(\\hat(\\beta_0)\\) \\[ Var(\\hat\\beta_1)= \\left( \\dfrac{s^2}{\\sum x_i ^2} \\right) \\] Referências "],
["função-lm.html", "Capítulo 2 Função lm()", " Capítulo 2 Função lm() A função lm() tem como objetivo realizar ajustes de modelos lineares. A chamada básica dessa função é ajuste &lt;- lm ( formula , data) onde formula - descreve o modelo a ser ajustado. data - data frame com os dados a serem usados no ajuste do modelo. ajuste - lista com resultados gerados pela função lm(). A formula é escrita como \\[ y \\sim x_1 + x_2 + \\ldots + x_k, \\] onde ~ - separa a variável dependente, à esquerda, das variáveis independentes, à direita. + - separa as variáveis independentes. Em uma formula, pode aparecer outros operadores além do +, esses estão na tabela abaixo: Operador Ação ~ Separa a variável dependente das variáveisl independentes:y ~ x + y + w. + Separa as v. independentes: : Denota a interação entre as variáveis independentes: y ~ x + z + x:z * Denota todas as interações possíveis entre as variáveis independentes: y ~ x * z * w é o modelo y ~ x + z + w + x:z + x:w + z:w + x:z:w. ^ Denota todas as interações até determinada potência: y ~(x + y + z)^2 é o modelo y ~ x + z + w + x:z +x:w + z:w. . Denota todas as variáveis do data frame, excluíndo a variável. dependente: y ~ . é o modelo y ~x + y + z. - Remove uma variável do modelo: y ~(x + y + z)^2 - x:w é o modelo y ~ x + z + w + x:z + z:w. -1 Remove o intercepto do modelo. I() Os elementos entre são interpretados aritmeticamente: y ~ x + I(z+w)^2 é o modelo y ~ y + h, onde h é a soma de z + w ao quadrado. função Funções matemáticas podem ser usadas pela fórmula: log(y) ~ x + z + w. O R disponibiliza uma lista de funções uteis em analíses de de regressão linear simples e ou múltiplas. Essas funções são aplicadas a lista gerada pela função lm(), que produzem informações adicionais baseadas no modelo ajustado. A tabela abaixo apresenta essas funções: Função Ação summary() Apresenta um relatório detalhado do modelo ajuste. coeficients() Lista os modelos do parâmetros (interceptos e coneficeintes angulares ) do modelo ajustado. confint() Produz um intervalo de confiânça para o modelo de parâmetros (por padrão 95%). fitted() Lista os valores preditos pelo modelo. residuals() Lista os resíduos gerados pelo modelo. anova() Gera uma tabela ANOVA para o modelo ajustado, um uma ANOVA para comparar dois modelos. vcov() Apresenta a matriz de covariância dos parâmetros do modelo. AIC() Apresenta o critério de informação de Akaike. plot() Produz gráficos para realizar a análise dos resíduos. predict() Calcula os valores preditos pelo modelo para uma novo data frame. "],
["regressão-linear-simples-no-r.html", "Capítulo 3 Regressão Linear Simples no R 3.1 Exemplo 1 3.2 Exemplo 2 3.3 Exemplo 3", " Capítulo 3 Regressão Linear Simples no R Em uma regressão linear simples a variável dependente Y é explicada por uma variável independente X. Caso o diagrama de dispersão X e Y apresente pontos dispostos em torno de uma linha reta, existe uma evidência de uma relação linear entre as variáveis analisadas. O exemplos I e II são adaptaçãos dos apresentados em (Melo and Peternelli 2013 , pp. 190-200) 3.1 Exemplo 1 Um engeinheiro civil coleta dados em laborátorio, a fim de estudar a dilatação de um pilar de concreto segundo a temperatura ambiente no local onde se encontra o pilar. São os dados: Temperatura (ºC) Dilatação (mm) 18 5 16 3 25 10 22 8 20 6 22 7 23 9 19 6 17 5 Algumas perguntas são comuns: Posso realizar um estudo de regressão? Qual o modelo usar? Qual a equação que relaciona a dilatação e a temperatura? A temperatura realmente exerce influencia na dilatação do pilar? E possível quantificar essa relação? As respostas procuradas podem ser encontradas realizando-se uma análise de regressão. Primeiro é criado um data frame com os dados de dilatação e temperatura. temp &lt;- c(18,16,25,22,20,21,23,19,17) dilat &lt;- c(5,3,10,8,6,7,9,6,5) dados &lt;- data.frame (dilat, temp) Inicialmente, o estudo de regressão pode ser feito com a escolha do modelo. Para auxiliar na escolha deste, visualizaremos os pontos em um gráfico de dispersão: library(ggplot2) ggplot(dados, aes(x=temp, y=dilat)) + geom_point()+ labs(x=&quot;Temperatura (ºC)&quot;, y=&quot;Dilatação (mm)&quot;)+ theme_classic() O diagrama sugere uma tendência linear dos dados. Montaremos, portanto, um modelo de regressão linear simples, pois existe apenas uma variável independente temp relaciada à variável dependente dilat. Montando o modelo: reglin &lt;- lm(dilat ~ temp, dados) reglin ## ## Call: ## lm(formula = dilat ~ temp, data = dados) ## ## Coefficients: ## (Intercept) temp ## -8.1710 0.7323 O modelo retorna duas informações: O valor do intercepto; e o valor do coeficiente de inclinação da reta. Esses valores são representados por \\(\\beta_0\\) e \\(\\beta_1\\), respectivamente. Assim podemos concluir que o modelo ajustado é: \\[ \\hat{Y} = \\hat{\\beta_0}+ \\hat{\\beta_1}X \\] é \\[ dilat = -8.1710 + 0.7323temp \\] onde a temperatura é dada em ºC e a dilatação em mm. Com o comando predict() podemos obter os valores calculados de dilat, de acordo com o modelo ajustado, para os valores de temp. Veja: predict(reglin) ## 1 2 3 4 5 6 7 8 ## 5.009677 3.545161 10.135484 7.938710 6.474194 7.206452 8.670968 5.741935 ## 9 ## 4.277419 O primeiro valor apresentado, ou seja, 5,009677, representa o que foi calculado para a dilatação quando a temperatura é 18ºC (a primeira do objeto temp), e assim sucessivamente até o último valor de temp, gerando o nove valores apresentados. Podemos também obter os resíduos associados a cada observação. Esses resíduos seriam simplesmente a diferença entre o valor observado e calculado correpondente a cada observação. Veja: resid(reglin) ## 1 2 3 4 5 6 ## -0.009677419 -0.545161290 -0.135483871 0.061290323 -0.474193548 -0.206451613 ## 7 8 9 ## 0.329032258 0.258064516 0.722580645 dilat[1]-predict(reglin)[1] ## 1 ## -0.009677419 A seguinte apresentação tabular poderia ser usada, resumindo as informações: result &lt;- data.frame ( dilat, temp, calculado = predict(reglin), residuo = resid(reglin) ) result ## dilat temp calculado residuo ## 1 5 18 5.009677 -0.009677419 ## 2 3 16 3.545161 -0.545161290 ## 3 10 25 10.135484 -0.135483871 ## 4 8 22 7.938710 0.061290323 ## 5 6 20 6.474194 -0.474193548 ## 6 7 21 7.206452 -0.206451613 ## 7 9 23 8.670968 0.329032258 ## 8 6 19 5.741935 0.258064516 ## 9 5 17 4.277419 0.722580645 Agora vamos plotar novamente os dados e acrescentar ao gráfico, além da reta de regressão ajustada, segmentos de reta representando os resídous, ou seja, os segmento vão dos valores observados (pontos) aos calculádos (reta). Veja: ggplot(result) + geom_line( aes(x=temp, y=calculado),color=&quot;blue&quot;)+ geom_segment( aes(x=temp, y=dilat, xend=temp, yend=calculado) )+ geom_point( aes(x=temp, y=calculado), colour=&quot;red&quot;)+ geom_point( aes(x=temp, y=dilat ), colour=&quot;red&quot;)+ labs(x=&quot;Temperatura (ºC)&quot;, y=&quot;Dilatação (mm)&quot;)+ theme_classic() Podemos também realizar uma análise de variância da regressão da seguinte forma: anova(reglin) ## Analysis of Variance Table ## ## Response: dilat ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## temp 1 36.938 36.938 201.4 2.048e-06 *** ## Residuals 7 1.284 0.183 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Por meio dessa análise podemos verificar que o coeficiente \\(\\beta_1\\) é significativo (p-value encontrado foi na ordem de 10-6), ou seja, a temperatura influência significativamente a dilatação. Com o comando summary() podemos obter muitas outras informaçãoes: summary(reglin) ## ## Call: ## lm(formula = dilat ~ temp, data = dados) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.54516 -0.20645 -0.00968 0.25806 0.72258 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -8.1710 1.0475 -7.801 0.000107 *** ## temp 0.7323 0.0516 14.191 2.05e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4283 on 7 degrees of freedom ## Multiple R-squared: 0.9664, Adjusted R-squared: 0.9616 ## F-statistic: 201.4 on 1 and 7 DF, p-value: 2.048e-06 O valore do coeficiente de determinação (R2) é apresentaso em Multiple R-squared: 0.9664 e representa o quanto da variação da dilatação pode ser explicada pela variação da temperatura. Uma vez que o valor encontrado foi quase de 97%, há indicação de que o modelo escolhido (linear) se ajusta bem aos dados. Estas informações permitem que e faça uma ampla análise do modelo, mas esse não é objetivo deste texto por enquanto. 3.2 Exemplo 2 Da mesma forma que o modelo linear, qualquer modelo de regressão polinomial pode ser obtido com o comando lm(), que vem do inglês linear models. Veja o exemplo a seguir. Os dados a seguir referem-se à produção de certa variedade de grãos (prod) em relação a quantidade de fertilizante aplicado na lavoura (fert). fert &lt;- c(10,20,30,40,50,60,70,80,90,100) prd &lt;- c(42,61,81,94,98,96,83,79,59,43) library(ggplot2) ggplot(data.frame(fert, prd), aes(x=fert, y=prd)) + geom_point()+ labs(x=&quot;Fetilizante&quot;, y=&quot;Produção&quot;)+ theme_classic() Pelo diagrama de dispersão, observa-se uma tendência quadratica nos dados. Dentro do comando lm(), abaixo, observe a necessidade de usarmos, como parte do modelo, o comando I(). Esse comando permite inserimos diretamente, no modelo, termos do tipo x2. reg&lt;- lm(prd~fert + I(fert^2)) reg ## ## Call: ## lm(formula = prd ~ fert + I(fert^2)) ## ## Coefficients: ## (Intercept) fert I(fert^2) ## 15.51667 2.95720 -0.02716 Para acrescentar a curva no gráfico anterior e acrescentar os segmentos de reta que representa os resíduo ggplot(data.frame(fert, prd, calculado = predict(reg)) ) + geom_line( aes(x=fert, y=calculado) ,color=&quot;blue&quot;)+ geom_point( aes(x=fert, y=calculado), colour=&quot;red&quot;)+ geom_point( aes(x=fert, y=prd ), colour=&quot;red&quot;)+ geom_segment( aes(x=fert, y=prd, xend=fert, yend=calculado), color=&quot;blue&quot; )+ labs(x=&quot;Fertilizante&quot;, y=&quot;Produção&quot;)+ theme_classic() Varias outra análises podem ser feitas, conforme as realizadas no regressão linear. veja uma delas: anova(reg) ## Analysis of Variance Table ## ## Response: prd ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## fert 1 7.6 7.6 0.5878 0.4683 ## I(fert^2) 1 3894.6 3894.6 302.2072 5.126e-07 *** ## Residuals 7 90.2 12.9 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Considere que se queira obter simplesmente uma ANOVA contendo a soma dos da regressão como um todo. Nesse caso, pode-se usar os seguintes comandos: X &lt;- cbind(fert, fert2 = fert^2) reg &lt;- lm(prd~X) anova(reg) ## Analysis of Variance Table ## ## Response: prd ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## X 2 3902.2 1951.09 151.4 1.734e-06 *** ## Residuals 7 90.2 12.89 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Compare o quadro da ANOVA com aquele anterior. Agora a fonte de variação X corresponde ao conjuto fert e I(fert^2) simultaneamente. A soma dos graus de liberdade (DF) e a soma dos quadrados (Sum Sq) do fert e I(fert^2) na primeira análise correspondem ao grau de liberdade e a soma dos quadrados de X na segunda análise. O uso de uma ou de outra forma de apresentação dos resultados dependerá do interesse do pesquisador. Outros modelos de regressão polinomial podem ser obtidos de maneira análoga. 3.3 Exemplo 3 3.3.1 Implementação no R Nesse exemplo, será utilizado um conjunto de dados em que a variável resposta (Y) é o tempo de reação a um certo estímulo, e a variável explicativa (X) é a idade do indivíduo. (Bussab and Morettin 1988) No R, dados em tabelas são objetos do tipo data frame, nos quais cada coluna corresponde a uma variável e cada linha corresponde a uma observação. dados &lt;- data.frame ( tempo = c(96,92,106,100,98,104,110,101,116,106, 109,100,112,105,118,108,113,112,127,117), idade = c(20,20,20,20,25,25,25,25,30,30,30,30, 35,35,35,35,40,40,40,40)) dados ## tempo idade ## 1 96 20 ## 2 92 20 ## 3 106 20 ## 4 100 20 ## 5 98 25 ## 6 104 25 ## 7 110 25 ## 8 101 25 ## 9 116 30 ## 10 106 30 ## 11 109 30 ## 12 100 30 ## 13 112 35 ## 14 105 35 ## 15 118 35 ## 16 108 35 ## 17 113 40 ## 18 112 40 ## 19 127 40 ## 20 117 40 3.3.2 Visualizaçao dos dados ggplot(dados, aes(x=idade,y=tempo))+ geom_point()+ theme_classic() É possível observar um crescimento nos valores da variável Tempo de acordo com o aumento dos valores da variável Idade. Portanto, esperamos que o efeito da Idade sobre o Tempo seja positivo: \\(β &gt; 0\\) 3.3.3 Ajuste do Modelo modelo &lt;- lm (tempo~idade, data=dados ) podemos simplesmente consultar as estimativas dos parâmetros: modelo$coefficients ## (Intercept) idade ## 80.5 0.9 E temos a equação da reta ajustada: \\[ E(Y ) = 80.5 + 0.9 ∗ idade. \\] Porém, como nossos parâmetros são estimativas pontuais, é interessante testar a sua significância , ou seja, com que nível de confiânça eu consigo afirmar que este efeito estimado é diferente de zero. O comando summary() poderá indicar se os seus parâmetros estimados são significativos ou não, ou seja, se é possível assumir que são diferentes de zero. Essa função também retorna a medida \\(R^2\\) (adjusted R-squared), que indica o quanto da variação presente nos dados está sendo explicada pela covariável. summary(modelo) ## ## Call: ## lm(formula = tempo ~ idade, data = dados) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.500 -4.125 -0.750 2.625 10.500 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 80.5000 5.4510 14.768 1.67e-11 *** ## idade 0.9000 0.1769 5.089 7.66e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.593 on 18 degrees of freedom ## Multiple R-squared: 0.5899, Adjusted R-squared: 0.5672 ## F-statistic: 25.9 on 1 and 18 DF, p-value: 7.662e-05 Podemos observar neste output as estimativas dos parâmetros, o erro padrão associado a cada estimativa, uma estatística t e um p-valor associado, resultado do teste t utilizado para saber se as estimativas são realmente diferentes de zero. Quanto mais asteriscos presentes ao lado do efeito estimado, maior o nível de confiança com que podemos afirmar que o efeito não é nulo. Quanto ao \\(R^2\\), ao utilizar apenas uma variável é normal que o valor não seja extremamente alto. De qualquer maneira, na prática, 0.56 é um valor bastante razoável. 3.3.4 Qualidade do Ajuste Visualização da reta obtida ggplot(dados, aes(x=idade, y= tempo))+ geom_point() + geom_smooth(method=&quot;lm&quot;, formula = y ~ x, se=TRUE)+ theme_classic() A reta estimada claramente não coincidirá com todos os nossos dados. As medidas de distância entre os dados observados e a reta estimada são chamadas resíduos. Os resíduos são utilizados para avaliar o ajuste do modelo, e a qualidade das estimativas feitas a partir dele. 3.3.5 Pressupostos do Modelo Assim como qualquer método estatístico, a Regressão Linear, para ser corretamente utilizada, precisa que os dados estejam de acordo com algumas condições assumidas pelo modelo: Normalidade dos Resíduos - É necessário que os resíduos gerados pelo ajuste da reta sigam distribuição Normal. Homocedasticidade - É necessário que a variância de Y seja constante para todos os valores de X Independência -É necessário que não exista estrutura de dependência entre os dados, para que os resíduos sejam independentes e identicamente distribuídos. É possível avaliar se as suposições acima estão sendo cumpridas através da Análise de Resíduos , que pode ser feita graficamente através dos comandos: par(mfrow=c(2,2)) plot(modelo, which=1:4, pch = 20) No primeiro gráfico, temos os resíduos em função dos valores estimados. Podemos utilizar este gráfico para observar a independência e a homocedasticidade, se os resíduos se distribuem de maneira razoavelmente aleatória e com mesma amplitude em torno do zero. No segundo gráfico, podemos avaliar a normalidade dos resíduos. A linha diagonal pontilhada representa a distribuição normal teórica, e os pontos a distribuição dos resíduos observada. Espera-se que não exista grande fuga dos pontos em relação à reta teórica. O terceiro gráfico pode ser avaliado da mesma maneira que o primeiro, observando a aleatoriedade e amplitude, desta vez dos resíduos padronizados. E o último gráfico permite visualizar as Distâncias de Cook das observações, uma medida de influência que pode indicar a presença de outliers quando possui valor maior do que 1. Quando a análise gráfica apresenta dúvidas, é possível também realizar testes estatísticos sobre os resíduos obtidos. 3.3.6 Teste de Bartlett O Teste de Bartlett para Homogeneidade de Variâncias pode ser utilizado quando existe mais de uma observação para cada valor de X, e retorna uma estatística de teste K e um p-valor associado. Quando menor que 0.05 podemos rejeitar a hipótese de homogeneidade de variâncias ao nível de 95% de confiança bartlett.test(data=dados, tempo ~idade) ## ## Bartlett test of homogeneity of variances ## ## data: tempo by idade ## Bartlett&#39;s K-squared = 0.29867, df = 4, p-value = 0.9899 Neste caso, a hipótese não foi rejeitada, portanto é seguro assumir que os resíduos sejam homocedásticos. 3.3.7 Teste de Shapiro-Wilk O Teste de Shapiro-Wilk para normalidade avalia a aderência dos resíduos à distribuição Normal. O p-valor se refere à hipótese de que os resíduos seguem de fato uma distribuição Normal, e essa hipótese é rejeitada, de modo geral, quando p é menor que 0.05 shapiro.test(modelo$residuals) ## ## Shapiro-Wilk normality test ## ## data: modelo$residuals ## W = 0.93762, p-value = 0.2161 Neste caso, a hipótese não foi rejeitada e é seguro assumir que os resíduos tenham distribuição normal. 3.3.8 Interpretação do Modelo Ajustado o modelo de forma satisfatória, agora podemos interpretá-lo. Como na prática não existe sentido em assumir valor zero para a idade, o parâmetro \\(\\beta_0\\) não é interpretado. Já o parâmetro associado à variável idade, assumiu valor 0.9. Este valor representa o acréscimo no valor da variável Tempo, a cada unidade de acréscimo na variável Idade. Portanto, ao envelhecer 1 ano, os indivíduos têm seu tempo de reação aumentado, em média, em 0.9 unidades. Ao envelhecer 2 anos, têm seu tempo de reação aumentado, em média, em 1.8 unidades, e assim por diante. Referências "],
["exercícios.html", "Capítulo 4 Exercícios", " Capítulo 4 Exercícios Os exercícios abaixos são adaptações de atividades contidas em (Magalhães and Lima 2002, 349 – 350). Os objetivos as serem alcançados com esta atividade é treinar: O preparo dos dados para uma regressão linear simples. Vizualizar a relação linear entre a variável dependente e a independente, por meio do gráfico de dispersão e pelo da correlação. Fazer o ajuste de um modelo e recuperar alguns resultados do modelo. As funções a serem utilizadas estão na tabela abaixo: Função Descrição cor() Calcular a correlação entre duas variáveis. lm() Ajustar modelos lineares. data.frame () Criar um data frame. predict() Obter os valores preditos por um modelo. resid() Obter os resíduos gerados por um modelo. ggplot() meio de gerar gráfico. Exercício 1 - Copie e execute os seguinte código: # Quinze pessoas selecionadas ao acaso, para cada uma delas foi per- # guntado o seu peso atual (X) e em seguida seu peso foi aferido em # uma balança (Y). As pessoas são são capazes de aferir o seu própio # peso? x &lt;- c(83, 57, 73, 76, 55, 60, 98, 74, 44, 82, 76, 67, 54, 60, 71) y &lt;- c(82, 58, 69, 70, 54, 62, 92, 75, 45, 81, 78, 65, 56, 63, 70) cor (x,y) dados &lt;- data.frame(x,y) library (ggplot2) ggplot(dados, aes(x=x, y=y))+ geom_point () + labs (x = &quot;Peso estimado.&quot;, y = &quot;Peso real.&quot;, title = &quot;Peso real em função do peso estimado.&quot;)+ theme_classic() modelo &lt;- lm (y ~ x) anova (modelo) summary (modelo) valor_predito &lt;- predict (modelo) residuo &lt;- resid (modelo) dados2 &lt;- data.frame (x, y, valor_predito, residuo) ggplot (dados2)+ geom_point(aes(x=x, y=y))+ geom_line(aes(x=x,y=valor_predito))+ labs (x = &quot;Peso estimado.&quot;, y = &quot;Peso real.&quot;, title = &quot;Reta estimada.&quot;)+ theme_classic() Exercício 2 - Uma indústria submete seus novos operários a um teste de aptidão (X) e três meses depois mede a produtividade destes operários (Y). Os resultados estão na tabela a seguir Operário Aptidão (X) Produtividade (Y) A 22 45 B 25 37 C 15 25 D 19 40 E 22 33 F 18 30 Crie o vetor aptidao com os dados da varíavel X e o vetor prod com os dados da variável Y. Calcule a correlação entre aptidao e prod com a função cor(). Faça um gráfico de dispersão com a abssiça representando a varíavel aptidao e a ordenada representando a variável prod. Crie o data frame dados com os vetores prod e aptidao. Use o comando ggplot() associado com a geometria `geom_point. Use a função lm() para criar o modelo reg que representa a regressão linear simples, na qual a variável dependente é a prod e a independente é a aptidao. Exercício 3 - Um estudo pretende avaliar o efeitos da obsidade na pressão sanguina. Para tanto, foram avaliados os pesos para 6 indivíduos e construída a variável X representando a razão entre o peso real e ideal. Estudos indicam que um modelo de regressão linear simples é adequado para essa situação. Os dados obtidos foram: Indivíduo Razão (X) Pressão sitólica (Y) 1 1,23 129 2 1,42 130 3 1,35 133 4 1,67 139 5 1,65 136 6 1,56 134 Crie o vetor razao com os dados da varíavel X e o vetor pressao com os dados da variável Y. Calcule a correlação entre a variável dependente e a independnete. Faça um gráfico de dispersão com a abssiça representando a varíavel razao e a ordenada representando a variável pressao. Use a função lm() para criar o modelo reg2 que representa a regressão linear simples, na qual a variável dependente é a razao e a independente é a pressao. Crie o vetor desvio com os devios observados (resid()) e o vetor previsto com os valores preditos pelo modelo (predict()). Como os vetores razao, pressao, desvio e previsto crie o data frame resultado. Com o data frame resultado crie um gráfico de dispersão que apresente a reta estimada e os pontos que representam os valores observados em razao e pressao. Referências "],
["regressão-no-r-relatório-.html", "Capítulo 5 Regressão no R - Relatório. 5.1 Relatório 5.2 Significância dos coeficientes 5.3 Estística t e p-valor: 5.4 Intervalos de confiança 5.5 Medidas de ajuste do modelo 5.6 Resumo", " Capítulo 5 Regressão no R - Relatório. O comando summary() quando atua sobre um modelo gerado pelos comando lm() produz um relatório que permite verificar aspectos importantes em um modelo linear como as estimativas dos parâmetros, testes de hipoteses sobre os parâmtros e medidas de qualidade do modelo. Neste capítulo será feito um estudo em cada resultado apresentado. 5.1 Relatório A fim de prosseguir esse estudo será realizado um ajuste do modelo para a seguinte situação. Acredita-se que a presença de uma substância na alimentação de bovinos melhora o ganho de peso desses animais. Para verificar essa hipótese, foram escolhidos 15 bois de mesma raça e idade, e cada um recebeu uma determinada concentração da substância X (mg/l). O ganho de peso após 30 dias, Y (kg), e a concetração atribuída foram as seguintes: X (concentração - mg/l) Y (ganho de peso - Kg) 0,2 9,4 0,5 11,4 0,6 12,3 0,7 10,2 1,0 11,9 1,5 13,6 2,0 14,2 2,5 16,2 3,0 16,2 3,5 17,7 4,0 18,8 4,5 19,9 5,0 22,5 5,5 24,7 6,0 23,1 Essas informações serão organizadas em um data frame chamado de dados, a partir desse data frame será calculada a correlação entre as varíaveis X e Y e gerá um gráfico de dispersão afim de observar o comportamento destes dados. dados &lt;- data.frame( X = c(0.2, 0.5, 0.6, 0.7, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0), Y = c( 9.4, 11.4, 12.3, 10.2, 11.9, 13.6, 14.2, 16.2, 16.2, 17.7, 18.8, 19.9, 22.5, 24.7, 23.1) ) correlacao &lt;- cor (dados$X, dados$Y) library(ggplot2, warn.conflicts = FALSE) ggplot(dados, aes(x=X, y=Y)) + geom_point()+ labs(x=&quot;Concentração (mg/l)&quot;, y=&quot;Ganho de peso (kg)&quot;)+ theme_classic() A correlação 0.98 e o gráfico de dispersão indicam que um modelo linear é uma boa opção. Agora: Ajustaremos o modelo \\(Y = \\beta_o + \\beta_1 X\\) por meio da função lm(). Faremos um gráfico de dispersão com os dados observados e a reta predita pelo modelo. Apresentaremos o relatório gerado pelo comando summary(). O ajuste do modelo será atribuído ao objeto modelo, que contém os valores preditos pelo modelo no componente fitted.values. modelo &lt;- lm(Y ~ X, dados) result &lt;- data.frame (dados, predito = modelo$fitted.values) ggplot(result) + geom_line( aes(x=X, y=predito))+ geom_point( aes(x=X, y=Y))+ labs(x=&quot;Concentração (mg/l)&quot;, y=&quot;Ganho de peso (kg)&quot;)+ theme_classic() Graficamente, a reta proposta se ajusta bem aos dados. O relatório a seguir nos dará informações para uma melhor avaliação do modelo. Esse relatório é gerado pelo comando R summary(): summary(modelo) ## ## Call: ## lm(formula = Y ~ X, data = dados) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0934 -0.6359 -0.2317 0.5885 1.7268 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.5509 0.3956 24.14 3.49e-12 *** ## X 2.4404 0.1199 20.36 3.03e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8812 on 13 degrees of freedom ## Multiple R-squared: 0.9696, Adjusted R-squared: 0.9673 ## F-statistic: 414.5 on 1 and 13 DF, p-value: 3.034e-11 O summary(modelo) apresenta 7 resultados: Call. A presenta a chamada da função usada para construir o modelo de regressão. Residuals. O relatório descreve de maneira sucinta a distribuiçãos dos resíduos, que deve ter média e mediana proximas de zero e o valor máximo e o valor mínimo aproximadamente iguais em valor absoluto. Pois, por definição, a distribuição dos resíduos tem distribuição normal com média zero. Coefficients. Apresenta os coeficientes do modelo e a sua significância estatística. O intercepto e as variáveis independentes que estão significativamente associada com a variável dependente são assinaladas com asterscos. Residual standard error (RSE). E uma indicação da variabilidade dos resíduos, é o desvio médio dos pontos observados em torno da linha de regressão ajustada. Este é o desvio padrão de erros. R-squared (R2). Informa a proporção da informação (variância) nos dados que o modelo consegue explicar. Adjusted R-squared. É um ajuste no R-squared (R2), pois ele tende a aumetar em duas situações: pela diminuição de observações, em uma regressão linear simples ou pelo aumento de variáveis independentes em uma regressão linear múltipla. F-statistic. E o teste que verifica se ao menos 1 (um) parãmetro ajustado é significativamente diferente de 0 (zero). 5.2 Significância dos coeficientes A tabela dos coeficeintes do relatório associado ao modelo estatístico é composta por: estimativa dos coeficientes do modelo standard errors (SE) que são medidas de precisão (acuracia) dos coeficientes. O SE representa a variabilidade de um dado coeficiente quando obtido de varias amostras. Também pode ser usado para obter intervalos de confiança e a estatística teste t. **estistica teste t* e o p-valor associados, que decide a significância do coeficiente. Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 9.5509 0.3956 24.14 3.49e-12 *** X 2.4404 0.1199 20.36 3.03e-11 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 5.3 Estística t e p-valor: Para uma variável dependente, a estatística t e o seu p-valor associado testa se a relação entre a variável independente e a dependentes é significante, isto é, se o coeficeinte da variável dependente é significativamente diferente de zero. As hipóteses testadas são: Hipótese nula (H0) : O coeficeinte é igual a zero. Hipotese alternativa (Ha): o coeficiente é diferente de zero. Para um coeficiente $ $, o teste t calcula a estatistica teste: \\[ t = \\dfrac{\\beta -0 }{SE(\\beta)}, \\] onde SE( \\(\\beta\\) ) é o erro padrão de \\(\\beta\\). A testatística teste \\(t\\) mede a quantidade de desvios padrões que há entre \\(\\beta\\) e zero. Grandes valores da estatística \\(t\\) geram pequenos valores para o p-valor. Altos valores da estatistica t, que são associados a baixos valores de p-valor, indicam que a variável dependente analisada é mais significante. A legenda abaixo da tabela dos significantes é usada para indicar o nível de significância \\(\\alpha\\) para se rejeita a hipótese nula: um asterisco indica que se rejeita \\(H_0\\) para \\(0.01 &lt; \\alpha &lt;= 0.05\\). Quanto mais asterisco, após o \\(p-valor\\), a variável independente é mais significante. Um coeficiente estatísticamente significante de uma variável dependnete indica que há uma relação entre essa variável e a variável independente. No exemplo analisado, os p-valor associado tanto ao intercepto quanto a variável X, são altamente significantes, que indica para a rejeição de \\(H_0\\) e favor de \\(H_a\\), que siginifica que o intercepto e a variável X influênciam na vairável dependente \\(Y\\). Para uma dada variável independente, o teste t podem também ser útil como critério de permanência, ou não, dessa variável no modelo. Altos valores da estatistica t, sempre associado a baixos valores do p-valor, indicam que a variável deve ser mantida no modelo, equanto baixo valores da estatistica t, sempre associado a altos valores do p-valor, conduzem para a retirada do modelo. 5.4 Intervalos de confiança O erro padrão mede a variabilidade dos coeficientes. Ele é usado para calcular os intervalos de confiança desses coeficientes. Por exemplo o intervalo de confiança, com nível de confiança de 95%, para um certo coeficiente \\(\\beta\\) é definido por \\[ \\beta +/- 2*SE(beta), \\] onde, o limite inferior é b1 = b1 - 2SE(b1) = 0.047 - 20.00269 = 0.042 o limite superior é b1 = b1 + 2SE(b1) = 0.047 + 20.00269 = 0.052 Assim, há uma probabilidade de 95% do intervalo [0.042, 0.052] conter o verdadeiro valor de b1. Similarmente um intervalo de comfianca de 95% pode ser construído para b0. Estes inntervalos são calculados pelo comando confint(): confint(modelo) ## 2.5 % 97.5 % ## (Intercept) 8.696207 10.405554 ## X 2.181454 2.699375 5.5 Medidas de ajuste do modelo Uma vez identificado, ao menos, uma variável dependente significante associada a variável independente. O diagnóstico do modelo para checar o ajusto do modelo aos dados. Este processamento é também conhecido por goodness-of-fit. A qualidade geral do modelo de regressão linear ajustado pode ser verificada por três quantidades, são apresentados no relatório gerado para o modelo: Residual Standard Error (RSE). R-squared (R2) and Adjusted R2 (R2 ajustado) F-statistic Residual standard error: 0.8812 on 13 degrees of freedom Multiple R-squared: 0.9696, Adjusted R-squared: 0.9673 F-statistic: 414.5 on 1 and 13 DF, p-value: 3.034e-11 5.5.1 Residual standard error (RSE). O RSE é uma indicação da variabilidade dos resíduos, é o desvio médio dos pontos observados em torno da linha de regressão ajustada. Este é o desvio padrão de erros. O RSE fornece uma medida dos dados que não podem ser explicados pelo modelo. Ao comparar dois modelos, o modelo com o RSE menor é uma boa indicação de que esse modelo se ajusta melhor aos dados. Dividir o RSE pelo valor médio da variável dependente fornecerá a taxa de erro de previsão, que deve ser a menor possível. No nosso exemplo, RSE = 0,8812, significando que os ganhos de peso observados divergem da linha de regressão verdadeira em aproximadamente 0.8812 unidades em média. Se um RSE de 0,8812 unidades é ou não um erro de previsão aceitável é subjetivo e depende do contexto do problema. No entanto, podemos calcular o erro percentual. Em nosso conjunto de dados, o valor médio de ganho de peso é 16,1400 e, portanto, o erro percentual é de 0,8812/ 16,1400 = 5.4596 %. mean(dados$Y) ## [1] 16.14 sigma(modelo)/mean(dados$Y)*100 ## [1] 5.459581 5.5.2 R2 e R2 ajustado O R-squared (R2) assume valor entre 0 e 1 e informa a proporção da informação (variância) nos dados que o modelo consegue explicar. O R2 mede o ajuste do modelo aos dados. Em uma regressão linear simples, o R2 é o coeficiente de correlação de Pearson ao quadrado. Um alto valor do R2 é um bom indicativo. Entretanto, o valor do R2 tende a aumentar com o aumento do número de variáveis independentes no modelo. Assim, em uma regressão linera múltipla, deve-se considera o valor do R2 ajustado, que penaliza o R2 para um grande número de variáveis independentes. Um R2 ajustado próximo de 1 indica que uma grande poroporção da variabilidade da variável resposta é explicada pelo modelo. Um valor próximo de 0 indica que o modelo de regressão não explica variabiliadade da variável dependente. 5.5.3 F-Statistic: O teste F é um teste de signifiância geral, e indica se ao menos uma variável independente tem o seu coeficiente diferente de zero. Em uma regressão linear simples, este teste não tem um real interesse, pois apenas replica a informação dada pelo teste t apresentado no relatório do modelo. De fato, a estatistica F é o quadrado da estatística t. No exemplo, 414.5 = 20.362. Isso é verdade para qualquer modelo com 1 grau de liberdade. A estatística F tem maior importância quando há varias variáveis independentes, como na regressão linear múltipla. Uma estatística F com valor alto corresponde a um p-valor estatisticamente significante (p &lt; 0.05). Em nosso exemplo, a estatística F é igual a 414.5 e produz um p-valor de 3.034e-11, que é muito significante. 5.6 Resumo Ajustar o modelo de regressão. Verificar se há, ao menos, uma variável independente está significativamente associada a variável dependente. Se uma oum mais variáveis dependentes são significativas, verificar a qualidedade do ajuste do modelo pela análise dos valores do RSE, do R2 e da Estatistica F. "],
["referências.html", "Referências", " Referências "]
]
